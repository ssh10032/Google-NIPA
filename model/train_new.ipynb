{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model Structure LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class skeletonLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_dim):\n",
    "        super(skeletonLSTM, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(128)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.layer_norm2 = nn.LayerNorm(256)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)\n",
    "        self.layer_norm3 = nn.LayerNorm(512)\n",
    "\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=512, num_layers=1, batch_first=True)\n",
    "        self.layer_norm4 = nn.LayerNorm(512)\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layers with Layer Normalization\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.layer_norm3(x)\n",
    "\n",
    "        x, (hn, cn) = self.lstm4(x)\n",
    "        x = self.layer_norm4(x)\n",
    "\n",
    "        # Pooling to summarize sequence information\n",
    "        x = torch.mean(x, dim=1)  # Mean pooling over the sequence\n",
    "\n",
    "        # Fully connected layers with ReLU and Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        embedding = self.fc2(x)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model Structure Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(head, self).__init__()\n",
    "\n",
    "        # Feedforward layers\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1)  # Output layer has 1 unit for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for probability output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "class head_128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(head_128, self).__init__()\n",
    "\n",
    "        # Feedforward layers\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output layer has 1 unit for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for probability output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "DataSet & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, segment_length):\n",
    "        self.root_dir = '/media/baebro/NIPA_data/Train/landmarks/'\n",
    "        self.segment_length = segment_length\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.timestamp_to_indices = {}\n",
    "\n",
    "        for dance_name in os.listdir(self.root_dir):\n",
    "            dance_path = os.path.join(self.root_dir, dance_name)\n",
    "            if os.path.isdir(dance_path):\n",
    "                for csv_file in os.listdir(dance_path):\n",
    "                    if csv_file.endswith(\"_angles.csv\"):\n",
    "                        file_path = os.path.join(dance_path, csv_file)\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        df = self.interpolate_missing_values(df)\n",
    "                        self.process_file(df, dance_name, file_path)\n",
    "\n",
    "    def interpolate_missing_values(self, df):\n",
    "        # Check for NaN or Inf values and interpolate\n",
    "        if df.isnull().values.any() or np.isinf(df.values).any():\n",
    "            df = df.replace([np.inf, -np.inf], np.nan)\n",
    "            df = df.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "            # Fill any remaining NaN values (e.g., at the start or end) with 0\n",
    "            df = df.fillna(0)\n",
    "        return df\n",
    "\n",
    "    def process_file(self, df, dance_name, file_path):\n",
    "        # Segment angle data and store with dance and timestamp\n",
    "        for i in range(0, len(df) - self.segment_length + 1, self.segment_length):\n",
    "            segment = df.iloc[i:i + self.segment_length].values\n",
    "            timestamp = i\n",
    "            if len(segment) == self.segment_length:\n",
    "                self.data.append((segment, dance_name, timestamp, file_path))\n",
    "                self.labels.append(dance_name)\n",
    "\n",
    "                # Positive pair dictionary\n",
    "                if (dance_name, timestamp) not in self.timestamp_to_indices:\n",
    "                    self.timestamp_to_indices[(dance_name, timestamp)] = []\n",
    "                self.timestamp_to_indices[(dance_name, timestamp)].append(len(self.data) - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        segment, dance_name, timestamp, _ = self.data[idx]\n",
    "        anchor = torch.tensor(segment, dtype=torch.float32)\n",
    "\n",
    "        # Check for NaN or Inf values in anchor\n",
    "        if torch.isnan(anchor).any() or torch.isinf(anchor).any():\n",
    "            raise ValueError(f\"NaN or Inf value detected in anchor segment at index {idx}\")\n",
    "\n",
    "        # Positive pair (same dance, same timestamp)\n",
    "        pos_indices = self.timestamp_to_indices.get((dance_name, timestamp), [])\n",
    "        if len(pos_indices) > 1:\n",
    "            pos_idx = np.random.choice([i for i in pos_indices if i != idx])\n",
    "        else:\n",
    "            pos_idx = idx  # fallback to self if no other positive sample available\n",
    "        pos_segment, _, _, _ = self.data[pos_idx]\n",
    "        pos = torch.tensor(pos_segment, dtype=torch.float32)\n",
    "\n",
    "        # Check for NaN or Inf values in positive segment\n",
    "        if torch.isnan(pos).any() or torch.isinf(pos).any():\n",
    "            raise ValueError(f\"NaN or Inf value detected in positive segment at index {pos_idx}\")\n",
    "\n",
    "        # Negative pair (different dance)\n",
    "        all_labels = list(set(self.labels))\n",
    "        neg_dance = np.random.choice([l for l in all_labels if l != dance_name])\n",
    "        neg_indices = [i for i in range(len(self.data)) if self.labels[i] == neg_dance]\n",
    "        if neg_indices:\n",
    "            neg_idx = np.random.choice(neg_indices)\n",
    "        else:\n",
    "            neg_idx = idx  # fallback to self if no negative sample available\n",
    "        neg_segment, _, _, _ = self.data[neg_idx]\n",
    "        neg = torch.tensor(neg_segment, dtype=torch.float32)\n",
    "\n",
    "        # Check for NaN or Inf values in negative segment\n",
    "        if torch.isnan(neg).any() or torch.isinf(neg).any():\n",
    "            raise ValueError(f\"NaN or Inf value detected in negative segment at index {neg_idx}\")\n",
    "\n",
    "        return anchor, pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##### Train Loss #####\n",
    "class TripletContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(TripletContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Normalize features\n",
    "        anchor, positive, negative = F.normalize(anchor, dim=1), F.normalize(positive, dim=1), F.normalize(negative,\n",
    "                                                                                                           dim=1)\n",
    "\n",
    "        # Calculate similarities\n",
    "        pos_sim = torch.exp(torch.sum(anchor * positive, dim=1) / self.temperature)  # Anchor-Positive similarity\n",
    "        neg_sim = torch.exp(torch.sum(anchor * negative, dim=1) / self.temperature)  # Anchor-Negative similarity\n",
    "\n",
    "        # Loss calculation: maximize anchor-positive similarity, minimize anchor-negative similarity\n",
    "        loss = -torch.log(pos_sim / (pos_sim + neg_sim)).mean()\n",
    "        # print(f'loss check: {pos_sim / (pos_sim + neg_sim)}')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:geg7cghh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment1</strong> at: <a href='https://wandb.ai/songoh/skeleton_lstm_new/runs/geg7cghh' target=\"_blank\">https://wandb.ai/songoh/skeleton_lstm_new/runs/geg7cghh</a><br/> View project at: <a href='https://wandb.ai/songoh/skeleton_lstm_new' target=\"_blank\">https://wandb.ai/songoh/skeleton_lstm_new</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241031_174601-geg7cghh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:geg7cghh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baebro/nipa_ws/nipaproj_ws/model/wandb/run-20241031_174849-qmkc79pl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songoh/skeleton_lstm_new/runs/qmkc79pl' target=\"_blank\">experiment1</a></strong> to <a href='https://wandb.ai/songoh/skeleton_lstm_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songoh/skeleton_lstm_new' target=\"_blank\">https://wandb.ai/songoh/skeleton_lstm_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songoh/skeleton_lstm_new/runs/qmkc79pl' target=\"_blank\">https://wandb.ai/songoh/skeleton_lstm_new/runs/qmkc79pl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Training: 100%|██████████| 89/89 [00:04<00:00, 18.17it/s, N_classification_loss=0.64, P_classification_loss=0.746, contrastive_loss=0.947, loss=3.03] \n",
      "Epoch [1/10] Validation: 100%|██████████| 23/23 [00:00<00:00, 65.09it/s, N_classification_loss=0.636, P_classification_loss=0.738, contrastive_loss=0.271, loss=2.33]\n",
      "Epoch [2/10] Training:  38%|███▊      | 34/89 [00:02<00:03, 16.66it/s, N_classification_loss=0.632, P_classification_loss=0.734, contrastive_loss=0.453, loss=2.5] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    116\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     train_loader_tqdm\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem(), contrastive_loss\u001b[38;5;241m=\u001b[39mloss1\u001b[38;5;241m.\u001b[39mitem(), P_classification_loss\u001b[38;5;241m=\u001b[39mloss2\u001b[38;5;241m.\u001b[39mitem(), N_classification_loss\u001b[38;5;241m=\u001b[39mloss3\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    122\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 학습률 조정\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[39m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[39m.\u001b[39m_opt_called \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49m\u001b[39m__get__\u001b[39;49m(opt, opt\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/optim/adam.py:197\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    191\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cuda_graph_capture_health_check()\n\u001b[1;32m    199\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/optim/optimizer.py:428\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_graph_capture_health_check\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[39m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[39m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[39m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    426\u001b[0m         \u001b[39mnot\u001b[39;00m is_compiling()\n\u001b[1;32m    427\u001b[0m         \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_built()\n\u001b[0;32m--> 428\u001b[0m         \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available()\n\u001b[1;32m    429\u001b[0m     ):\n\u001b[1;32m    430\u001b[0m         capturing \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_current_stream_capturing()\n\u001b[1;32m    432\u001b[0m         \u001b[39mif\u001b[39;00m capturing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    433\u001b[0m             group[\u001b[39m\"\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups\n\u001b[1;32m    434\u001b[0m         ):\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/cuda/__init__.py:120\u001b[0m, in \u001b[0;36mis_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_compiled():\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[39mif\u001b[39;00m _nvml_based_avail():\n\u001b[1;32m    121\u001b[0m     \u001b[39m# The user has set an env variable to request this availability check that attempts to avoid fork poisoning by\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# using NVML at the cost of a weaker CUDA availability assessment. Note that if NVML discovery/initialization\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# fails, this assessment falls back to the default CUDA Runtime API assessment (`cudaGetDeviceCount`)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m device_count() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[39m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m# API via `cuInit`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/site-packages/torch/cuda/__init__.py:113\u001b[0m, in \u001b[0;36m_nvml_based_avail\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_nvml_based_avail\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mPYTORCH_NVML_BASED_CUDA_CHECK\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/os.py:776\u001b[0m, in \u001b[0;36mgetenv\u001b[0;34m(key, default)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetenv\u001b[39m(key, default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    773\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get an environment variable, return None if it doesn't exist.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m    The optional second argument can specify an alternate default.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m    key, default and the result are str.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m environ\u001b[39m.\u001b[39;49mget(key, default)\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/_collections_abc.py:824\u001b[0m, in \u001b[0;36mMapping.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39m'\u001b[39m\u001b[39mD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    823\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[1;32m    825\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/os.py:677\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m    676\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencodekey(key)]\n\u001b[1;32m    678\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m         \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m    680\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mp/lib/python3.10/os.py:755\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[39m# Where Env Var Names Can Be Mixed Case\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     encoding \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mgetfilesystemencoding()\n\u001b[0;32m--> 755\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(value):\n\u001b[1;32m    756\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    757\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mstr expected, not \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Train code #####\n",
    "wandb.init(project=\"skeleton_lstm_new\", name=\"experiment1\")\n",
    "save_path = '/home/baebro/nipa_ws/nipaproj_ws/output/'\n",
    "# 하이퍼파라미터 설정\n",
    "feature_dim = 12\n",
    "output_dim = 64\n",
    "num_epochs = 10  # Early stopping 적용 시 더 큰 값을 설정해도 됩니다\n",
    "learning_rate = 0.0001\n",
    "temperature = 0.1\n",
    "patience = 25  # Early stopping patience 설정\n",
    "min_delta = 0.001  # Validation loss가 감소하는 최소 값\n",
    "sequence_length = 30\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# 데이터셋과 데이터로더\n",
    "dataset = ContrastiveDataset(sequence_length)\n",
    "# print(len(dataset))\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Subset을 사용하여 학습 및 검증 데이터셋 생성\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=0,  # CPU 데드락 방지를 위해 num_workers=0으로 설정\n",
    "    batch_size=8,\n",
    "    shuffle=True,  # 데이터 순서를 섞어서 학습 효과를 높임\n",
    "    pin_memory=True  # GPU 사용 시 유용\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    num_workers=0,  # CPU 데드락 방지를 위해 num_workers=0으로 설정\n",
    "    batch_size=8,  # 검증도 충분한 배치 크기로 설정\n",
    "    shuffle=False,\n",
    "    pin_memory=True  # GPU 사용 시 유용\n",
    ")\n",
    "print()\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저, 스케줄러 초기화\n",
    "# model = skeletonLSTM(feature_dim, output_dim).to(device)\n",
    "model = skeletonLSTM(feature_dim, output_dim).to(device)\n",
    "# binary classification head\n",
    "classification = head_128().to(device)\n",
    "\n",
    "# criterion1 = ContrastiveLoss(margin=1.0)\n",
    "criterion1 = TripletContrastiveLoss(temperature=temperature)\n",
    "\n",
    "# Binary classification loss\n",
    "criterion2 = nn.BCELoss()\n",
    "criterion3 = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Early stopping 변수 초기화\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    classification.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    p_epoch_loss = 0.0\n",
    "    n_epoch_loss = 0.0\n",
    "    # epoch_classification_loss = 0.0\n",
    "    train_loader_tqdm = tqdm(train_dataloader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] Training\")\n",
    "    for batch_idx, (anchor, pos, neg) in enumerate(train_loader_tqdm):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 데이터를 장치로 이동\n",
    "        anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n",
    "\n",
    "        # 모델에 통과하여 임베딩 생성 및 이진 분류 출력\n",
    "        anchor_emb = model(anchor)\n",
    "        pos_emb = model(pos)\n",
    "        neg_emb = model(neg)\n",
    "\n",
    "        # Contrastive Loss 계산\n",
    "        # anchor_emb = F.normalize(anchor_emb, dim=1)\n",
    "        # pos_emb = F.normalize(pos_emb, dim=1)\n",
    "        # neg_emb = F.normalize(neg_emb, dim=1)\n",
    "\n",
    "        # positive_dist = F.pairwise_distance(anchor_emb, pos_emb)\n",
    "        # negative_dist = F.pairwise_distance(anchor_emb, neg_emb)\n",
    "        # loss1 = criterion1(positive_dist, negative_dist)\n",
    "\n",
    "        loss1 = criterion1(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "        # Classification Loss 계산\n",
    "        pos_classification = classification(torch.cat((anchor_emb, pos_emb), dim=1))\n",
    "        loss2 = criterion2(pos_classification, torch.full((pos_classification.shape[0], 1), 1.).to(device))\n",
    "\n",
    "        neg_classification = classification(torch.cat((anchor_emb, neg_emb), dim=1))\n",
    "        loss3 = criterion3(neg_classification, torch.full((neg_classification.shape[0], 1), 0.).to(device))\n",
    "\n",
    "        # Total loss 계산\n",
    "        loss = loss1 + 1.5 * (loss2 + loss3)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        p_epoch_loss += loss2.item()\n",
    "        n_epoch_loss += loss3.item()\n",
    "        # epoch_classification_loss += classification_loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item(), contrastive_loss=loss1.item(), P_classification_loss=loss2.item(), N_classification_loss=loss3.item())\n",
    "\n",
    "\n",
    "    scheduler.step()  # 학습률 조정\n",
    "\n",
    "    # 에폭마다 평균 손실을 기록\n",
    "    avg_train_contrastive_loss = loss1.item() / len(train_dataloader)\n",
    "    avg_train_Pos_bc_loss = p_epoch_loss / len(train_dataloader)\n",
    "    avg_train_Neg_bc_loss = n_epoch_loss / len(train_dataloader)\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "    # avg_classification_loss = epoch_classification_loss / len(train_dataloader)\n",
    "    # print(\n",
    "    #     f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, P_Classification Loss: {avg_train_Pos_bc_loss:.4f}, N_Classification Loss: {avg_train_Neg_bc_loss:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_p_loss = 0.0\n",
    "    val_n_loss = 0.0\n",
    "    # val_classification_loss = 0.0\n",
    "    val_loader_tqdm = tqdm(val_dataloader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] Validation\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (anchor, pos, neg) in enumerate(val_loader_tqdm):\n",
    "            anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n",
    "\n",
    "            # 임베딩 생성 및 이진 분류 출력\n",
    "            anchor_emb = model(anchor)\n",
    "            pos_emb = model(pos)\n",
    "            neg_emb = model(neg)\n",
    "\n",
    "            # Contrastive Loss 계산\n",
    "            # anchor_emb = F.normalize(anchor_emb, dim=1)\n",
    "            # pos_emb = F.normalize(pos_emb, dim=1)\n",
    "            # neg_emb = F.normalize(neg_emb, dim=1)\n",
    "            # positive_dist = F.pairwise_distance(anchor_emb, pos_emb)\n",
    "            # negative_dist = F.pairwise_distance(anchor_emb, neg_emb)\n",
    "            # loss1 = criterion1(positive_dist, negative_dist)\n",
    "\n",
    "            loss1 = criterion1(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "            # Classification Loss 계산\n",
    "            pos_classification = classification(torch.cat((anchor_emb, pos_emb), dim=1))\n",
    "            loss2 = criterion2(pos_classification, torch.full((pos_classification.shape[0], 1), 1.).to(device))\n",
    "\n",
    "            neg_classification = classification(torch.cat((anchor_emb, neg_emb), dim =1))\n",
    "            loss3 = criterion3(neg_classification, torch.full((neg_classification.shape[0], 1), 0.).to(device))\n",
    "\n",
    "            # Total validation loss 계산\n",
    "            loss = loss1 + 1.5 * (loss2 + loss3)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_p_loss += loss2.item()\n",
    "            val_n_loss += loss3.item()\n",
    "\n",
    "            val_loader_tqdm.set_postfix(loss=loss.item(), contrastive_loss=loss1.item(), P_classification_loss=loss2.item(), N_classification_loss=loss3.item())\n",
    "\n",
    "    # avg_train_contrastive_loss = loss1.item() / len(train_dataloader)\n",
    "    # avg_train_Pos_bc_loss = loss2.item() / len(train_dataloader)\n",
    "    # avg_train_Neg_bc_loss = loss3.item() / len(train_dataloader)\n",
    "\n",
    "    # print(\n",
    "    #     f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_contrastive_loss:.4f}, P_Classification Loss: {avg_train_Pos_bc_loss:.4f}, N_Classification Loss: {avg_train_Neg_bc_loss:.4f}')\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    # avg_val_classification_loss = val_classification_loss / len(val_dataloader)\n",
    "\n",
    "    # avg_val_loss = val_loss.item() / len(train_dataloader)\n",
    "    avg_val_Pos_bc_loss = val_p_loss / len(train_dataloader)\n",
    "    avg_val_Neg_bc_loss = val_n_loss / len(train_dataloader)\n",
    "\n",
    "    # print(\n",
    "    #     f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, P_Classification Loss: {avg_val_Pos_bc_loss:.4f}, , N_Classification Loss: {avg_val_Neg_bc_loss:.4f}')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0  # Improvement이 있으면 카운트 리셋\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        early_stop = True\n",
    "\n",
    "    # 에폭마다 평균 손실을 기록\n",
    "    wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_train_loss,\n",
    "               \"val_loss\": avg_val_loss})\n",
    "\n",
    "print(\"Training complete!\")\n",
    "# torch.save(model.state_dict(), save_path + 'model_state_dict_Contrastive_loss_w_bc.pt')\n",
    "# torch.save(classification.state_dict(), save_path + 'model_state_dict_Contrastive_loss_w_bc.pt')\n",
    "torch.save(model, save_path + 'lstm_2000_cat_ov5.pth')\n",
    "torch.save(classification, save_path + 'head_2000_cat_ov5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_angle(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    segment = df.iloc[:30].values\n",
    "    segment = torch.tensor(segment, dtype=torch.float32).unsqueeze(0).to(torch.float32)\n",
    "    return segment\n",
    "\n",
    "def read_angles(file_path, sequence_length):\n",
    "    angle_list = []\n",
    "    df = pd.read_csv(file_path)\n",
    "    for i in range(0, 300, sequence_length):\n",
    "        angle_list.append(torch.tensor(df.iloc[i:i + sequence_length].values, dtype=torch.float32))\n",
    "    pos1_tensor = torch.stack(angle_list, dim=0).to(device)\n",
    "    return pos1_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saved_path = '/home/baebro/nipa_ws/nipaproj_ws/output/'\n",
    "model_name = 'lstm_2000_cat_ov5.pth'\n",
    "head_name = 'head_2000_cat_ov5.pth'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "origin_path = '/home/baebro/nipa_ws/nipaproj_ws/sample_videos/labeled_data/pos1_infer/landmarks_3d_pos1_infer_angles.csv'\n",
    "pos_path = '/home/baebro/nipa_ws/nipaproj_ws/sample_videos/labeled_data/pos2_infer/landmarks_3d_pos2_infer_angles.csv'\n",
    "neg_path = '/home/baebro/nipa_ws/nipaproj_ws/sample_videos/labeled_data/neg_infer/landmarks_3d_neg_infer_angles.csv'\n",
    "\n",
    "origin_input = read_angles(origin_path, 30).to(device)\n",
    "pos_input = read_angles(pos_path, 30).to(device)\n",
    "neg_input = read_angles(neg_path, 30).to(device)\n",
    "\n",
    "model = torch.load(saved_path+model_name).to(device)\n",
    "head = torch.load(saved_path+head_name).to(device)\n",
    "model.eval()\n",
    "head.eval()\n",
    "\n",
    "origin_emb = model(origin_input)\n",
    "pos_emb = model(pos_input)\n",
    "neg_emb =model(neg_input)\n",
    "\n",
    "origin_emb_norm = F.normalize(origin_emb, dim=1)\n",
    "pos_emb_norm = F.normalize(pos_emb, dim=1)\n",
    "neg_emb_norm = F.normalize(neg_emb, dim=1)\n",
    "\n",
    "pos_dist = 1-torch.pow(F.pairwise_distance(origin_emb_norm, pos_emb_norm), 1)/2\n",
    "neg_dist = 1-torch.pow(F.pairwise_distance(origin_emb_norm, neg_emb_norm), 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(neg_dist)\n",
    "print(pos_dist)\n",
    "print(neg_dist/pos_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos_classification = head(torch.cat((origin_emb, pos_emb), dim=1))\n",
    "neg_classification = head(torch.cat((origin_emb, neg_emb), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(neg_classification)\n",
    "print(pos_classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 ('mp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "75fe21128fcfacce69d1e29e597ca9ba6f67d76a760b48a77174c70425a216bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
