{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-29T01:28:36.147879Z","iopub.status.busy":"2024-10-29T01:28:36.147082Z","iopub.status.idle":"2024-10-29T01:28:42.542021Z","shell.execute_reply":"2024-10-29T01:28:42.541063Z","shell.execute_reply.started":"2024-10-29T01:28:36.147837Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:42.544661Z","iopub.status.busy":"2024-10-29T01:28:42.544006Z","iopub.status.idle":"2024-10-29T01:28:42.557572Z","shell.execute_reply":"2024-10-29T01:28:42.556677Z","shell.execute_reply.started":"2024-10-29T01:28:42.544615Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","    \n","seed_everything(12)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:42.559080Z","iopub.status.busy":"2024-10-29T01:28:42.558711Z","iopub.status.idle":"2024-10-29T01:28:42.567957Z","shell.execute_reply":"2024-10-29T01:28:42.566899Z","shell.execute_reply.started":"2024-10-29T01:28:42.559036Z"},"trusted":true},"outputs":[],"source":["class skeleton_LSTM(nn.Module):\n","    def __init__(self, feature_dim, output_dim):\n","        super(skeleton_LSTM, self).__init__()\n","        \n","        self.feature_dim = feature_dim\n","        self.output_dim = output_dim\n","        self.lstm1 = nn.LSTM(input_size=self.feature_dim, hidden_size=128, num_layers=1, batch_first=True)\n","        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)\n","        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)\n","        self.fc1 = nn.Linear(512,256)\n","        self.fc2 = nn.Linear(256,output_dim)\n","        \n","    def forward(self, x):\n","        x, _ = self.lstm1(x)\n","        x, _ = self.lstm2(x)\n","        x, (hn, cn) = self.lstm3(x)\n","        x = F.relu(self.fc1(x[:,-1,:]))\n","        embedding = self.fc2(x)\n","        \n","        return embedding\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:42.570977Z","iopub.status.busy":"2024-10-29T01:28:42.570571Z","iopub.status.idle":"2024-10-29T01:28:42.585749Z","shell.execute_reply":"2024-10-29T01:28:42.584604Z","shell.execute_reply.started":"2024-10-29T01:28:42.570935Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","\n","angle = [['left_biceps', 'left_forearm'],\n","         ['right_biceps', 'right_forearm'],\n","         ['between_shoulders', 'left_body'],\n","         ['between_shoulders', 'right_body'],\n","         ['between_shoulders', 'rigth_neck'],\n","         ['between_shoulders', 'left_neck'],\n","         ['between_pelvis','left_thigh'],\n","         ['between_pelvis','right_thigh'],\n","         ['right_thigh','right_calf'],\n","         ['left_thigh','left_calf'],\n","         ['right_body','right_thigh'],\n","         ['left_body','left_thigh']\n","        ]\n","         \n","\n","body_parts = {'left_biceps': [11, 13],\n","              'left_forearm': [13, 15],\n","              'right_biceps': [12, 14],\n","              'right_forearm': [14, 16],\n","              'between_shoulders': [11, 12],\n","              'left_body': [11, 23],\n","              'right_body': [12, 24],\n","              'between_pelvis': [23, 24],\n","              'left_thigh': [23, 25],\n","              'left_calf': [25, 27],\n","              'right_thigh': [24, 26],\n","              'right_calf': [26, 28],\n","              'left_neck': [9, 11],\n","              'rigth_neck': [10, 12]}\n","\n","\n","def calculate_angles(matrix1, matrix2):\n","    dot_product = np.einsum('ij,ij->i', matrix1, matrix2)\n","    norm1 = np.linalg.norm(matrix1, axis=1)\n","    norm2 = np.linalg.norm(matrix2, axis=1)\n","    cos_theta = dot_product / (norm1 * norm2)\n","    angles = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n","    return angles\n","\n","\n","def make_df_angle(path):\n","    df = pd.read_csv(path)\n","    df_angle = pd.DataFrame()\n","\n","    for body_parts1, body_parts2 in angle:\n","        body_parts1_vec = body_parts[body_parts1]\n","        body_parts2_vec = body_parts[body_parts2]\n","\n","        # 벡터 계산\n","        vec_mat1 = df.iloc[:, body_parts1_vec[0]*3+1:body_parts1_vec[0]*3+4].values - df.iloc[:, body_parts1_vec[1]*3+1:body_parts1_vec[1]*3+4].values\n","        vec_mat2 = df.iloc[:, body_parts2_vec[0]*3+1:body_parts2_vec[0]*3+4].values - df.iloc[:, body_parts2_vec[1]*3+1:body_parts2_vec[1]*3+4].values\n","\n","        angles = calculate_angles(vec_mat1, vec_mat2)\n","        df_angle[f'{body_parts1}_{body_parts2}'] = angles\n","        \n","    df_angle = df_angle.replace([np.inf, -np.inf], 0.0)\n","    df_angle = df_angle.fillna(0.0)\n","\n","\n","    return df_angle\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:42.587734Z","iopub.status.busy":"2024-10-29T01:28:42.587329Z","iopub.status.idle":"2024-10-29T01:28:42.603344Z","shell.execute_reply":"2024-10-29T01:28:42.602571Z","shell.execute_reply.started":"2024-10-29T01:28:42.587700Z"},"trusted":true},"outputs":[],"source":["class LandmarkDataset(Dataset):\n","    def __init__(self, segment_length):\n","        self.root_dir = '/media/baebro/NIPA_data/Train/landmarks'\n","        self.data = []\n","        self.labels = []\n","        self.label_to_indices = {}\n","        self.segment_length = segment_length\n","\n","        # 디렉토리 탐색\n","        for dance_name in os.listdir(self.root_dir):\n","            dance_path = os.path.join(self.root_dir, dance_name)\n","            if os.path.isdir(dance_path):\n","                for csv_file in os.listdir(dance_path):\n","                    if csv_file.endswith(\".csv\") and not csv_file.endswith(\"_F.csv\"):\n","                        file_path = os.path.join(dance_path, csv_file)\n","                        df_angle = make_df_angle(file_path)\n","                        \n","                        # 시퀀스를 segment_length 단위로 나누어 저장\n","                        for i in range(0, len(df_angle) - self.segment_length + 1, self.segment_length):\n","                            segment = df_angle.iloc[i:i + self.segment_length].values\n","                            self.data.append(segment)\n","                            self.labels.append(dance_name)\n","\n","                            # 인덱스를 라벨에 매핑\n","                            if dance_name not in self.label_to_indices:\n","                                self.label_to_indices[dance_name] = []\n","                            self.label_to_indices[dance_name].append(len(self.data) - 1)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        label = self.labels[idx]\n","        anchor = torch.tensor(self.data[idx], dtype=torch.float32)\n","\n","        # Positive 쌍 생성\n","        positive_idx = np.random.choice(self.label_to_indices[label])\n","        while positive_idx == idx:\n","            positive_idx = np.random.choice(self.label_to_indices[label])\n","        pos = torch.tensor(self.data[positive_idx], dtype=torch.float32)\n","\n","        # Negative 쌍 생성\n","        neg_label = np.random.choice([l for l in self.label_to_indices if l != label])\n","        negative_idx = np.random.choice(self.label_to_indices[neg_label])\n","        neg = torch.tensor(self.data[negative_idx], dtype=torch.float32)\n","\n","        return anchor, pos, neg\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:42.604888Z","iopub.status.busy":"2024-10-29T01:28:42.604570Z","iopub.status.idle":"2024-10-29T01:28:42.618214Z","shell.execute_reply":"2024-10-29T01:28:42.617270Z","shell.execute_reply.started":"2024-10-29T01:28:42.604844Z"},"trusted":true},"outputs":[],"source":["class TripletContrastiveLoss(nn.Module):\n","    def __init__(self, temperature=0.1):\n","        super(TripletContrastiveLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, anchor, positive, negative):\n","        # Normalize features\n","        anchor, positive, negative = F.normalize(anchor, dim=1), F.normalize(positive, dim=1), F.normalize(negative, dim=1)\n","        \n","        # Calculate similarities\n","        pos_sim = torch.exp(torch.sum(anchor * positive, dim=1) / self.temperature)  # Anchor-Positive similarity\n","        neg_sim = torch.exp(torch.sum(anchor * negative, dim=1) / self.temperature)  # Anchor-Negative similarity\n","\n","        # Loss calculation: maximize anchor-positive similarity, minimize anchor-negative similarity\n","        loss = -torch.log(pos_sim / (pos_sim + neg_sim)).mean()\n","        return loss\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:42.621111Z","iopub.status.busy":"2024-10-29T01:28:42.620736Z","iopub.status.idle":"2024-10-29T01:28:43.554080Z","shell.execute_reply":"2024-10-29T01:28:43.553190Z","shell.execute_reply.started":"2024-10-29T01:28:42.621069Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Subset\n","\n","# 데이터셋과 데이터로더\n","dataset = LandmarkDataset()\n","train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n","\n","\n","# Subset을 사용하여 학습 및 검증 데이터셋 생성\n","train_dataset = Subset(dataset, train_indices)\n","val_dataset = Subset(dataset, val_indices)\n","\n","# DataLoader 생성\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:28:43.555486Z","iopub.status.busy":"2024-10-29T01:28:43.555173Z","iopub.status.idle":"2024-10-29T01:32:06.824457Z","shell.execute_reply":"2024-10-29T01:32:06.823460Z","shell.execute_reply.started":"2024-10-29T01:28:43.555449Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Epoch [1/100], Training Loss: 0.5617\n","Epoch [1/100], Validation Loss: 0.6931\n","Epoch [2/100], Training Loss: 0.6931\n","Epoch [2/100], Validation Loss: 0.6931\n","Epoch [3/100], Training Loss: 0.6931\n","Epoch [3/100], Validation Loss: 0.6931\n","Epoch [4/100], Training Loss: 0.6903\n","Epoch [4/100], Validation Loss: 0.6931\n","Epoch [5/100], Training Loss: 0.6894\n","Epoch [5/100], Validation Loss: 0.6138\n","Epoch [6/100], Training Loss: 0.6603\n","Epoch [6/100], Validation Loss: 0.6931\n","Epoch [7/100], Training Loss: 0.6931\n","Epoch [7/100], Validation Loss: 0.6931\n","Epoch [8/100], Training Loss: 0.6931\n","Epoch [8/100], Validation Loss: 0.6931\n","Epoch [9/100], Training Loss: 0.6931\n","Epoch [9/100], Validation Loss: 0.6931\n","Epoch [10/100], Training Loss: 0.6931\n","Epoch [10/100], Validation Loss: 0.6931\n","Epoch [11/100], Training Loss: 0.6931\n","Epoch [11/100], Validation Loss: 0.6931\n","Epoch [12/100], Training Loss: 0.6931\n","Epoch [12/100], Validation Loss: 0.6931\n","Epoch [13/100], Training Loss: 0.6931\n","Epoch [13/100], Validation Loss: 0.6931\n","Epoch [14/100], Training Loss: 0.6931\n","Epoch [14/100], Validation Loss: 0.6931\n","Epoch [15/100], Training Loss: 0.6931\n","Epoch [15/100], Validation Loss: 0.6931\n","Early stopping triggered!\n","Training complete!\n"]}],"source":["import torch\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","\n","# 하이퍼파라미터 설정\n","feature_dim = len(angle)\n","output_dim = 64\n","num_epochs = 100  # Early stopping 적용 시 더 큰 값을 설정해도 됩니다\n","learning_rate = 0.001\n","temperature = 0.1\n","patience = 10  # Early stopping patience 설정\n","min_delta = 0.001  # Validation loss가 감소하는 최소 값\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","# 모델, 손실 함수, 옵티마이저, 스케줄러 초기화\n","model = skeleton_LSTM(feature_dim, output_dim).to(device)\n","criterion = TripletContrastiveLoss(temperature=temperature)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Early stopping 변수 초기화\n","best_val_loss = np.inf\n","epochs_no_improve = 0\n","early_stop = False\n","\n","# 학습 루프\n","for epoch in range(num_epochs):\n","    if early_stop:\n","        break\n","\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch_idx, (anchor, pos, neg) in enumerate(train_dataloader):\n","        optimizer.zero_grad()\n","        \n","        # 데이터를 장치로 이동\n","        anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n","        \n","        # 모델에 통과하여 임베딩 생성\n","        anchor_emb = model(anchor)\n","        pos_emb = model(pos)\n","        neg_emb = model(neg)\n","\n","        # Contrastive Loss 계산\n","        loss = criterion(anchor_emb, pos_emb, neg_emb)\n","        epoch_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","    \n","    scheduler.step()  # 학습률 조정\n","\n","    # 에폭마다 평균 손실을 기록\n","    avg_train_loss = epoch_loss / len(train_dataloader)\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}')\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for batch_idx, (anchor, pos, neg) in enumerate(val_dataloader):\n","            anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n","            \n","            # 임베딩 생성\n","            anchor_emb = model(anchor)\n","            pos_emb = model(pos)\n","            neg_emb = model(neg)\n","            \n","            # Validation loss 계산\n","            loss = criterion(anchor_emb, pos_emb, neg_emb)\n","            val_loss += loss.item()\n","    \n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n","\n","    # Early Stopping 체크\n","    if avg_val_loss < best_val_loss - min_delta:\n","        best_val_loss = avg_val_loss\n","        epochs_no_improve = 0  # Improvement이 있으면 카운트 리셋\n","    else:\n","        epochs_no_improve += 1\n","\n","    if epochs_no_improve >= patience:\n","        print(\"Early stopping triggered!\")\n","        early_stop = True\n","\n","print(\"Training complete!\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:32:06.826554Z","iopub.status.busy":"2024-10-29T01:32:06.825811Z","iopub.status.idle":"2024-10-29T01:32:07.254090Z","shell.execute_reply":"2024-10-29T01:32:07.253197Z","shell.execute_reply.started":"2024-10-29T01:32:06.826505Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0042], device='cuda:0', grad_fn=<PowBackward0>)\n","tensor([0.0126], device='cuda:0', grad_fn=<PowBackward0>)\n"]},{"data":{"text/plain":["tensor([2.9700], device='cuda:0', grad_fn=<DivBackward0>)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# 유클리디언 거리 계산\n","model.eval()\n","\n","origin_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/Hurricane Venus (보아)/landmarks_3d_L.csv'\n","pos_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/Hurricane Venus (보아)/landmarks_3d_P.csv'\n","neg_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/View (샤이니)/landmarks_3d_P.csv'\n","\n","origin = make_df_angle(origin_path)\n","pos = make_df_angle(pos_path)\n","neg = make_df_angle(neg_path)\n","\n","origin_input = torch.tensor(origin.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","pos_input = torch.tensor(pos.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","neg_input = torch.tensor(neg.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","\n","origin_emb = model(origin_input)\n","pos_emb = model(pos_input)\n","neg_emb = model(neg_input)\n","\n","pos_dist = torch.pow(F.pairwise_distance(origin_emb, pos_emb), 2)\n","neg_dist = torch.pow(F.pairwise_distance(origin_emb, neg_emb), 2)\n","print(pos_dist)\n","print(neg_dist)\n","neg_dist/pos_dist"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:32:07.256881Z","iopub.status.busy":"2024-10-29T01:32:07.256565Z","iopub.status.idle":"2024-10-29T01:32:07.355724Z","shell.execute_reply":"2024-10-29T01:32:07.354814Z","shell.execute_reply.started":"2024-10-29T01:32:07.256849Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0683], device='cuda:0', grad_fn=<PowBackward0>)\n","tensor([0.0971], device='cuda:0', grad_fn=<PowBackward0>)\n"]},{"data":{"text/plain":["tensor([1.4211], device='cuda:0', grad_fn=<DivBackward0>)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 정규화 후, 유클리디언 거리 계산\n","model.eval()\n","\n","origin_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/Hurricane Venus (보아)/landmarks_3d_L.csv'\n","pos_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/Hurricane Venus (보아)/landmarks_3d_P.csv'\n","neg_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/ID Peace B (보아)/landmarks_3d_P.csv'\n","\n","origin = make_df_angle(origin_path)\n","pos = make_df_angle(pos_path)\n","neg = make_df_angle(neg_path)\n","\n","origin_input = torch.tensor(origin.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","pos_input = torch.tensor(pos.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","neg_input = torch.tensor(neg.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","\n","origin_emb = F.normalize(model(origin_input), dim=1)\n","pos_emb = F.normalize(model(pos_input), dim=1)\n","neg_emb = F.normalize(model(neg_input), dim=1)\n","\n","pos_dist = torch.pow(F.pairwise_distance(origin_emb, pos_emb), 2)\n","neg_dist = torch.pow(F.pairwise_distance(origin_emb, neg_emb), 2)\n","print(pos_dist)\n","print(neg_dist)\n","neg_dist/pos_dist"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:32:26.281742Z","iopub.status.busy":"2024-10-29T01:32:26.281264Z","iopub.status.idle":"2024-10-29T01:32:26.369510Z","shell.execute_reply":"2024-10-29T01:32:26.368484Z","shell.execute_reply.started":"2024-10-29T01:32:26.281702Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.9658], device='cuda:0', grad_fn=<SumBackward1>)\n","tensor([0.9515], device='cuda:0', grad_fn=<SumBackward1>)\n"]}],"source":["# 코사인 유사도 계산\n","\n","model.eval()\n","\n","origin_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/Hurricane Venus (보아)/landmarks_3d_L.csv'\n","pos_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/Hurricane Venus (보아)/landmarks_3d_P.csv'\n","neg_path = '/kaggle/input/gmb-nipa/landmarks/landmarks/ID Peace B (보아)/landmarks_3d_P.csv'\n","\n","origin = make_df_angle(origin_path)\n","pos = make_df_angle(pos_path)\n","neg = make_df_angle(neg_path)\n","\n","origin_input = torch.tensor(origin.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","pos_input = torch.tensor(pos.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","neg_input = torch.tensor(neg.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","\n","origin_emb = F.normalize(model(origin_input), dim=1)\n","pos_emb = F.normalize(model(pos_input), dim=1)\n","neg_emb = F.normalize(model(neg_input), dim=1)\n","\n","pos_sim = torch.sum(origin_emb * pos_emb, dim=1)\n","neg_sim = torch.sum(origin_emb * neg_emb, dim=1)\n","\n","print(pos_sim)\n","print(neg_sim)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T01:32:07.357281Z","iopub.status.busy":"2024-10-29T01:32:07.356970Z","iopub.status.idle":"2024-10-29T01:32:07.390175Z","shell.execute_reply":"2024-10-29T01:32:07.389231Z","shell.execute_reply.started":"2024-10-29T01:32:07.357250Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.02616405487060547"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from time import time\n","\n","model.eval()\n","\n","start = time()\n","origin_path = '/kaggle/input/nipa-sample/sample_video/100 (슈퍼엠)/landmarks_3d_L.csv'\n","origin = make_df_angle(origin_path)\n","origin_input = torch.tensor(origin.iloc[:44,].values).unsqueeze(0).to(torch.float32).to(device)\n","\n","origin_emb = F.normalize(model(origin_input),dim=1)\n","time()-start"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5961287,"sourceId":9739372,"sourceType":"datasetVersion"},{"datasetId":5962620,"sourceId":9741204,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.10.15 ('mp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"vscode":{"interpreter":{"hash":"75fe21128fcfacce69d1e29e597ca9ba6f67d76a760b48a77174c70425a216bc"}}},"nbformat":4,"nbformat_minor":4}
